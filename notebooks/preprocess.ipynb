{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748e73787913fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 60)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de9a9f4878832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wildfire_df = pd.read_csv(\"./../data/raw/california-wildfires/b8aeb030-140d-43d2-aa29-1a80862e3d62.csv\", low_memory=False)\n",
    "\n",
    "wildfire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79305dd5652619be",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_df = pd.read_csv(\"./../data/raw/us-air-quality/US_AQI.csv\", low_memory=False)\n",
    "\n",
    "neighboring_states = [\"CA\", \"OR\", \"NV\", \"AZ\"]  # State codes\n",
    "aqi_neighbors_df = aqi_df[aqi_df[\"state_id\"].isin(neighboring_states)]\n",
    "\n",
    "aqi_neighbors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85debae22bed997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_missing_data_heatmap(df):\n",
    "\n",
    "    # Create a more informative heatmap\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Calculate missing data percentage\n",
    "    missing_percentage = (df.isnull().mean() * 100).sort_values(ascending=False)\n",
    "\n",
    "    # Create a dataframe for missing data\n",
    "    missing_df = pd.DataFrame({'Column': missing_percentage.index,\n",
    "                              'Missing %': missing_percentage.values})\n",
    "\n",
    "    # Plot\n",
    "    sns.heatmap(df[missing_df['Column']].isnull(),\n",
    "                cbar=False,\n",
    "                cmap='coolwarm',\n",
    "                yticklabels=False)\n",
    "    plt.title('Missing Data Heatmap', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print missing percentage for reference\n",
    "    print(\"\\nMissing Data Percentage by Column:\")\n",
    "    display(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502991ae3a80770",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_data_heatmap(wildfire_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941341489c7a415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_data_heatmap(aqi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "def get_location_details(latitude, longitude):\n",
    "    \"\"\"\n",
    "    Get detailed location information from coordinates using reverse geocoding.\n",
    "    \n",
    "    Args:\n",
    "        latitude (float): Latitude coordinate\n",
    "        longitude (float): Longitude coordinate\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing location details or None if not found\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"wildfire_aqi_analysis\")\n",
    "    \n",
    "    try:\n",
    "        location = geolocator.reverse(f\"{latitude}, {longitude}\", exactly_one=True, language='en')\n",
    "        \n",
    "        if not location:\n",
    "            return None\n",
    "        \n",
    "        address = location.raw.get('address', {})\n",
    "        \n",
    "        # Extract different components\n",
    "        details = {\n",
    "            'city': address.get('city') or address.get('town') or address.get('village'),\n",
    "            'county': address.get('county') or address.get('municipality'),\n",
    "            'state': address.get('state') or address.get('province'),\n",
    "            'country': address.get('country'),\n",
    "            'postcode': address.get('postcode'),\n",
    "            'full_address': location.address\n",
    "        }\n",
    "        \n",
    "        return details\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding {latitude}, {longitude}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "latitude = 40.7128  # New York City coordinates\n",
    "longitude = -74.0060\n",
    "\n",
    "location_info = get_location_details(latitude, longitude)\n",
    "\n",
    "if location_info:\n",
    "    print(f\"City: {location_info['city']}\")\n",
    "    print(f\"County: {location_info['county']}\")\n",
    "    print(f\"State: {location_info['state']}\")\n",
    "    print(f\"Country: {location_info['country']}\")\n",
    "    print(f\"Full Address: {location_info['full_address']}\")\n",
    "else:\n",
    "    print(\"Location not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ddcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supplement_wildfire_missing_data(df, lat_col='LATITUDE', lon_col='LONGITUDE', \n",
    "                                   county_col='COUNTY', batch_size=50, delay=1):\n",
    "    \"\"\"\n",
    "    Supplement missing county information in wildfire data using reverse geocoding.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Wildfire dataframe\n",
    "        lat_col (str): Name of latitude column\n",
    "        lon_col (str): Name of longitude column\n",
    "        county_col (str): Name of county column to supplement\n",
    "        batch_size (int): Number of records to process before showing progress\n",
    "        delay (float): Delay between API calls to respect rate limits\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with supplemented data\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Find rows with missing county data but valid coordinates\n",
    "    missing_county = df_copy[county_col].isna()\n",
    "    has_coordinates = df_copy[lat_col].notna() & df_copy[lon_col].notna()\n",
    "    to_supplement = missing_county & has_coordinates\n",
    "    \n",
    "    if not to_supplement.any():\n",
    "        print(\"No records need county supplementation\")\n",
    "        return df_copy\n",
    "    \n",
    "    print(f\"Found {to_supplement.sum()} records with missing county data that have coordinates\")\n",
    "    \n",
    "    count = 0\n",
    "    for idx in df_copy[to_supplement].index:\n",
    "        lat = df_copy.loc[idx, lat_col]\n",
    "        lon = df_copy.loc[idx, lon_col]\n",
    "        \n",
    "        location_info = get_location_details(lat, lon)\n",
    "        \n",
    "        if location_info and location_info['county']:\n",
    "            df_copy.loc[idx, county_col] = location_info['county']\n",
    "            count += 1\n",
    "        \n",
    "        # Progress update and rate limiting\n",
    "        if (count + 1) % batch_size == 0:\n",
    "            print(f\"Processed {count + 1} records...\")\n",
    "        \n",
    "        time.sleep(delay)  # Rate limiting\n",
    "    \n",
    "    print(f\"Successfully supplemented {count} county records\")\n",
    "    return df_copy\n",
    "\n",
    "def supplement_aqi_missing_data(df, lat_col='Latitude', lon_col='Longitude', \n",
    "                               city_col='City', county_col='County', state_col='State',\n",
    "                               batch_size=50, delay=1):\n",
    "    \"\"\"\n",
    "    Supplement missing location information in AQI data using reverse geocoding.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): AQI dataframe\n",
    "        lat_col (str): Name of latitude column\n",
    "        lon_col (str): Name of longitude column\n",
    "        city_col (str): Name of city column to supplement\n",
    "        county_col (str): Name of county column to supplement\n",
    "        state_col (str): Name of state column to supplement\n",
    "        batch_size (int): Number of records to process before showing progress\n",
    "        delay (float): Delay between API calls to respect rate limits\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with supplemented data\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Find rows with missing location data but valid coordinates\n",
    "    has_coordinates = df_copy[lat_col].notna() & df_copy[lon_col].notna()\n",
    "    missing_any_location = (df_copy[city_col].isna() | \n",
    "                           df_copy[county_col].isna() | \n",
    "                           df_copy[state_col].isna())\n",
    "    to_supplement = missing_any_location & has_coordinates\n",
    "    \n",
    "    if not to_supplement.any():\n",
    "        print(\"No records need location supplementation\")\n",
    "        return df_copy\n",
    "    \n",
    "    print(f\"Found {to_supplement.sum()} records with missing location data that have coordinates\")\n",
    "    \n",
    "    count = 0\n",
    "    for idx in df_copy[to_supplement].index:\n",
    "        lat = df_copy.loc[idx, lat_col]\n",
    "        lon = df_copy.loc[idx, lon_col]\n",
    "        \n",
    "        location_info = get_location_details(lat, lon)\n",
    "        \n",
    "        if location_info:\n",
    "            # Only update if the field is currently missing\n",
    "            if pd.isna(df_copy.loc[idx, city_col]) and location_info['city']:\n",
    "                df_copy.loc[idx, city_col] = location_info['city']\n",
    "            \n",
    "            if pd.isna(df_copy.loc[idx, county_col]) and location_info['county']:\n",
    "                df_copy.loc[idx, county_col] = location_info['county']\n",
    "            \n",
    "            if pd.isna(df_copy.loc[idx, state_col]) and location_info['state']:\n",
    "                df_copy.loc[idx, state_col] = location_info['state']\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "        # Progress update and rate limiting\n",
    "        if (count + 1) % batch_size == 0:\n",
    "            print(f\"Processed {count + 1} records...\")\n",
    "        \n",
    "        time.sleep(delay)  # Rate limiting\n",
    "    \n",
    "    print(f\"Successfully processed {count} location records\")\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ff418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what columns we have in our datasets for geocoding supplementation\n",
    "print(\"=== WILDFIRE DATASET COLUMNS ===\")\n",
    "print(\"Available columns:\")\n",
    "print(wildfire_df.columns.tolist())\n",
    "print(f\"\\nShape: {wildfire_df.shape}\")\n",
    "\n",
    "print(\"\\n=== AQI DATASET COLUMNS ===\")\n",
    "print(\"Available columns:\")\n",
    "print(aqi_neighbors_df.columns.tolist())\n",
    "print(f\"\\nShape: {aqi_neighbors_df.shape}\")\n",
    "\n",
    "# Check for coordinate columns and location columns that could be supplemented\n",
    "print(\"\\n=== GEOCODING OPPORTUNITIES ===\")\n",
    "\n",
    "# For wildfire data\n",
    "wildfire_coords = []\n",
    "wildfire_locations = []\n",
    "for col in wildfire_df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if any(coord in col_lower for coord in ['lat', 'lon', 'coord']):\n",
    "        wildfire_coords.append(col)\n",
    "    elif any(loc in col_lower for loc in ['county', 'city', 'state', 'location']):\n",
    "        wildfire_locations.append(col)\n",
    "\n",
    "print(f\"Wildfire coordinate columns: {wildfire_coords}\")\n",
    "print(f\"Wildfire location columns: {wildfire_locations}\")\n",
    "\n",
    "# For AQI data\n",
    "aqi_coords = []\n",
    "aqi_locations = []\n",
    "for col in aqi_neighbors_df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if any(coord in col_lower for coord in ['lat', 'lon', 'coord']):\n",
    "        aqi_coords.append(col)\n",
    "    elif any(loc in col_lower for loc in ['county', 'city', 'state', 'location']):\n",
    "        aqi_locations.append(col)\n",
    "\n",
    "print(f\"\\nAQI coordinate columns: {aqi_coords}\")\n",
    "print(f\"AQI location columns: {aqi_locations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data in location columns before supplementation\n",
    "print(\"=== MISSING DATA ANALYSIS FOR GEOCODING ===\")\n",
    "\n",
    "print(\"\\nWildfire Dataset - Missing Location Data:\")\n",
    "wildfire_location_missing = {\n",
    "    'City': wildfire_df['* City'].isna().sum(),\n",
    "    'County': wildfire_df['County'].isna().sum(), \n",
    "    'State': wildfire_df['State'].isna().sum(),\n",
    "    'Has_Coordinates': (~wildfire_df['Latitude'].isna() & ~wildfire_df['Longitude'].isna()).sum()\n",
    "}\n",
    "\n",
    "for key, value in wildfire_location_missing.items():\n",
    "    print(f\"  {key}: {value:,} missing/available\")\n",
    "\n",
    "print(f\"\\nRecords with coordinates but missing county: {(wildfire_df['County'].isna() & ~wildfire_df['Latitude'].isna() & ~wildfire_df['Longitude'].isna()).sum():,}\")\n",
    "\n",
    "print(\"\\nAQI Dataset - Missing Location Data:\")\n",
    "aqi_location_missing = {\n",
    "    'City': aqi_neighbors_df['city_ascii'].isna().sum(),\n",
    "    'State_Name': aqi_neighbors_df['state_name'].isna().sum(),\n",
    "    'Has_Coordinates': (~aqi_neighbors_df['lat'].isna() & ~aqi_neighbors_df['lng'].isna()).sum()\n",
    "}\n",
    "\n",
    "for key, value in aqi_location_missing.items():\n",
    "    print(f\"  {key}: {value:,} missing/available\")\n",
    "\n",
    "# Note: AQI dataset doesn't seem to have a county column, so we could add one using geocoding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41159b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate geocoding supplementation on a small sample\n",
    "print(\"=== GEOCODING DEMONSTRATION ===\")\n",
    "\n",
    "# Test the geocoding function first\n",
    "print(\"Testing geocoding function with sample coordinates...\")\n",
    "sample_lat, sample_lon = 34.0522, -118.2437  # Los Angeles coordinates\n",
    "test_location = get_location_details(sample_lat, sample_lon)\n",
    "print(f\"Sample location info: {test_location}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WILDFIRE DATA SUPPLEMENTATION DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find records with missing county data\n",
    "missing_county_mask = wildfire_df['County'].isna() & ~wildfire_df['Latitude'].isna() & ~wildfire_df['Longitude'].isna()\n",
    "missing_county_records = wildfire_df[missing_county_mask]\n",
    "\n",
    "if len(missing_county_records) > 0:\n",
    "    print(f\"Found {len(missing_county_records)} records with missing county but valid coordinates\")\n",
    "    print(\"\\nSample records with missing county:\")\n",
    "    print(missing_county_records[['* City', 'County', 'State', 'Latitude', 'Longitude']].head())\n",
    "    \n",
    "    # Demonstrate on just 2-3 records to avoid rate limiting during demo\n",
    "    print(f\"\\nDemonstrating county supplementation on first 2 records...\")\n",
    "    demo_sample = missing_county_records.head(2).copy()\n",
    "    \n",
    "    for idx, row in demo_sample.iterrows():\n",
    "        print(f\"\\nProcessing record {idx}:\")\n",
    "        print(f\"  Current: City='{row['* City']}', County='{row['County']}', State='{row['State']}'\")\n",
    "        print(f\"  Coordinates: {row['Latitude']}, {row['Longitude']}\")\n",
    "        \n",
    "        location_info = get_location_details(row['Latitude'], row['Longitude'])\n",
    "        if location_info:\n",
    "            print(f\"  Geocoded county: '{location_info['county']}'\")\n",
    "            print(f\"  Full address: {location_info['full_address']}\")\n",
    "        else:\n",
    "            print(\"  Could not geocode this location\")\n",
    "        \n",
    "        time.sleep(1)  # Rate limiting\n",
    "else:\n",
    "    print(\"No records found with missing county data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AQI DATA ENHANCEMENT DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# The AQI dataset doesn't have county information - we could add it!\n",
    "print(\"AQI dataset could benefit from adding county information using coordinates\")\n",
    "print(\"\\nSample AQI records:\")\n",
    "aqi_sample = aqi_neighbors_df[['city_ascii', 'state_name', 'lat', 'lng']].head(3)\n",
    "print(aqi_sample)\n",
    "\n",
    "print(f\"\\nDemonstrating county addition for first AQI record...\")\n",
    "first_aqi = aqi_neighbors_df.iloc[0]\n",
    "print(f\"Current: City='{first_aqi['city_ascii']}', State='{first_aqi['state_name']}'\")\n",
    "print(f\"Coordinates: {first_aqi['lat']}, {first_aqi['lng']}\")\n",
    "\n",
    "location_info = get_location_details(first_aqi['lat'], first_aqi['lng'])\n",
    "if location_info:\n",
    "    print(f\"Could add county: '{location_info['county']}'\")\n",
    "    print(f\"Full address: {location_info['full_address']}\")\n",
    "else:\n",
    "    print(\"Could not geocode this AQI location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTIONS FOR FULL DATASET GEOCODING\n",
    "print(\"=\"*60)\n",
    "print(\"FULL DATASET GEOCODING INSTRUCTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "To apply geocoding to your full datasets, use the following code:\n",
    "\n",
    "1. SUPPLEMENT WILDFIRE MISSING COUNTY DATA:\n",
    "   # This will fill in missing county information for wildfire records\n",
    "   wildfire_supplemented = supplement_wildfire_missing_data(\n",
    "       wildfire_df, \n",
    "       lat_col='Latitude', \n",
    "       lon_col='Longitude', \n",
    "       county_col='County',\n",
    "       batch_size=25,  # Process 25 records at a time\n",
    "       delay=1.5       # Wait 1.5 seconds between API calls\n",
    "   )\n",
    "\n",
    "2. ADD COUNTY COLUMN TO AQI DATA:\n",
    "   # First, add a county column to the AQI dataframe\n",
    "   aqi_neighbors_df['County'] = None\n",
    "   \n",
    "   # Then supplement with geocoding (this will take a while for full dataset!)\n",
    "   aqi_with_counties = supplement_aqi_missing_data(\n",
    "       aqi_neighbors_df,\n",
    "       lat_col='lat',\n",
    "       lon_col='lng', \n",
    "       city_col='city_ascii',\n",
    "       county_col='County',  # New column we just added\n",
    "       state_col='state_name',\n",
    "       batch_size=25,\n",
    "       delay=1.5\n",
    "   )\n",
    "\n",
    "3. SAVE PROCESSED DATA:\n",
    "   # Save the supplemented datasets\n",
    "   wildfire_supplemented.to_csv('../data/processed/california-wildfires/wildfires_with_locations.csv', index=False)\n",
    "   aqi_with_counties.to_csv('../data/processed/us-air-quality/aqi_with_counties.csv', index=False)\n",
    "\n",
    "IMPORTANT NOTES:\n",
    "- Geocoding API calls have rate limits (usually 1 request per second for Nominatim)\n",
    "- Processing large datasets will take considerable time\n",
    "- Consider processing in smaller batches and saving intermediate results\n",
    "- The full AQI dataset has 817k records - this could take 10+ hours to fully geocode\n",
    "- Consider filtering to specific date ranges or regions first to reduce processing time\n",
    "\n",
    "RATE LIMITING CONSIDERATIONS:\n",
    "- Nominatim allows 1 request per second maximum\n",
    "- For 817k AQI records at 1.5 second intervals = ~340 hours of processing time\n",
    "- Consider using paid geocoding services for large datasets (Google Maps API, MapBox, etc.)\n",
    "\"\"\")\n",
    "\n",
    "# Show the current data shapes and potential processing times\n",
    "wildfire_missing = (wildfire_df['County'].isna() & \n",
    "                   ~wildfire_df['Latitude'].isna() & \n",
    "                   ~wildfire_df['Longitude'].isna()).sum()\n",
    "\n",
    "aqi_total = len(aqi_neighbors_df)\n",
    "\n",
    "print(f\"\\nCURRENT DATASET STATUS:\")\n",
    "print(f\"- Wildfire records needing county supplementation: {wildfire_missing:,}\")\n",
    "print(f\"- AQI records that could get county data: {aqi_total:,}\")\n",
    "print(f\"\\nESTIMATED PROCESSING TIME (at 1.5 sec per record):\")\n",
    "print(f\"- Wildfire supplementation: {(wildfire_missing * 1.5 / 60):.1f} minutes\")\n",
    "print(f\"- AQI county addition: {(aqi_total * 1.5 / 3600):.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d26b3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 records with missing county data that have coordinates\n",
      "Processed 25 records...\n",
      "Successfully supplemented 30 county records\n"
     ]
    }
   ],
   "source": [
    "wildfire_supplemented = supplement_wildfire_missing_data(\n",
    "    wildfire_df, \n",
    "    lat_col='Latitude', \n",
    "    lon_col='Longitude', \n",
    "    county_col='County',\n",
    "    batch_size=25,  # Process 25 records at a time\n",
    "    delay=1.5       # Wait 1.5 seconds between API calls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33c74bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved supplemented data to: ../data/processed/california-wildfires/wildfires_supplemented.csv\n",
      "Loaded existing data from: ../data/processed/california-wildfires/wildfires_with_locations.csv\n",
      "\n",
      "COMPARISON:\n",
      "Original data shape: (100230, 47)\n",
      "Supplemented data shape: (100230, 47)\n",
      "Existing processed data shape: (100230, 20)\n",
      "\n",
      "COLUMN COMPARISON:\n",
      "Original columns: 47\n",
      "Supplemented columns: 47\n",
      "Existing processed columns: 20\n",
      "New columns in existing: set()\n",
      "Removed columns from existing: {'Street Suffix (e.g. apt. 23, blding C)', '* Roof Construction', '* Vent Screen', 'Battalion', 'Distance - Residence to Utility/Misc Structure &gt; 120 SQFT', '# of Damaged Outbuildings < 120 SQFT', 'If Affected 1-9% - Where did fire start?', '* City', 'Distance - Propane Tank to Structure', 'Assessed Improved Value (parcel)', '* Street Number', '* Street Name', 'Structure Defense Actions Taken', '* Eaves', '* Window Pane', '* Street Type (e.g. road, drive, lane, etc.)', 'Community', 'Site Address (parcel)', '# of Non Damaged Outbuildings < 120 SQFT', 'Year Built (parcel)', '* Fence Attached to Structure', '* Exterior Siding', 'Zip Code', 'APN (parcel)', '# Units in Structure (if multi unit)', 'If Affected 1-9% - What started fire?', 'Fire Name (Secondary)'}\n",
      "\n",
      "COUNTY DATA COMPARISON:\n",
      "Original missing counties: 30\n",
      "Supplemented missing counties: 0\n",
      "Existing processed missing counties: 0\n",
      "Counties filled by supplementation: 30\n"
     ]
    }
   ],
   "source": [
    "# Save supplemented wildfire data\n",
    "output_path = \"../data/processed/california-wildfires/wildfires_supplemented.csv\"\n",
    "wildfire_supplemented.to_csv(output_path, index=False)\n",
    "print(f\"Saved supplemented data to: {output_path}\")\n",
    "\n",
    "# Load existing processed data for comparison\n",
    "existing_path = \"../data/processed/california-wildfires/wildfires_with_locations.csv\"\n",
    "try:\n",
    "    existing_df = pd.read_csv(existing_path)\n",
    "    print(f\"Loaded existing data from: {existing_path}\")\n",
    "    \n",
    "    # Compare datasets\n",
    "    print(f\"\\nCOMPARISON:\")\n",
    "    print(f\"Original data shape: {wildfire_df.shape}\")\n",
    "    print(f\"Supplemented data shape: {wildfire_supplemented.shape}\")\n",
    "    print(f\"Existing processed data shape: {existing_df.shape}\")\n",
    "    \n",
    "    # Compare column differences\n",
    "    original_cols = set(wildfire_df.columns)\n",
    "    supplemented_cols = set(wildfire_supplemented.columns)\n",
    "    existing_cols = set(existing_df.columns)\n",
    "    \n",
    "    print(f\"\\nCOLUMN COMPARISON:\")\n",
    "    print(f\"Original columns: {len(original_cols)}\")\n",
    "    print(f\"Supplemented columns: {len(supplemented_cols)}\")\n",
    "    print(f\"Existing processed columns: {len(existing_cols)}\")\n",
    "    \n",
    "    if supplemented_cols != original_cols:\n",
    "        print(f\"New columns in supplemented: {supplemented_cols - original_cols}\")\n",
    "        print(f\"Removed columns from supplemented: {original_cols - supplemented_cols}\")\n",
    "    \n",
    "    if existing_cols != original_cols:\n",
    "        print(f\"New columns in existing: {existing_cols - original_cols}\")\n",
    "        print(f\"Removed columns from existing: {original_cols - existing_cols}\")\n",
    "    \n",
    "    # Compare missing data in County column\n",
    "    original_missing_county = wildfire_df['County'].isna().sum()\n",
    "    supplemented_missing_county = wildfire_supplemented['County'].isna().sum()\n",
    "    existing_missing_county = existing_df['County'].isna().sum() if 'County' in existing_df.columns else \"N/A\"\n",
    "    \n",
    "    print(f\"\\nCOUNTY DATA COMPARISON:\")\n",
    "    print(f\"Original missing counties: {original_missing_county:,}\")\n",
    "    print(f\"Supplemented missing counties: {supplemented_missing_county:,}\")\n",
    "    print(f\"Existing processed missing counties: {existing_missing_county}\")\n",
    "    print(f\"Counties filled by supplementation: {original_missing_county - supplemented_missing_county}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"No existing processed file found at: {existing_path}\")\n",
    "    print(\"This appears to be the first processed version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fcd9383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED COMPARISON:\n",
      "\n",
      "Sample records where counties were supplemented:\n",
      "\n",
      "Record 78435:\n",
      "  Original County: nan\n",
      "  Supplemented County: Yuba County\n",
      "  City: Brownsville \n",
      "  State: CA\n",
      "  Coordinates: (39.4098639835184, -121.306040400213)\n",
      "\n",
      "Record 78436:\n",
      "  Original County: nan\n",
      "  Supplemented County: Yuba County\n",
      "  City: Brownsville \n",
      "  State: CA\n",
      "  Coordinates: (39.4097826500257, -121.305075083185)\n",
      "\n",
      "Record 78437:\n",
      "  Original County: nan\n",
      "  Supplemented County: Yuba County\n",
      "  City: Brownsville \n",
      "  State: CA\n",
      "  Coordinates: (39.4098022003337, -121.304769832957)\n",
      "\n",
      "Record 78438:\n",
      "  Original County: nan\n",
      "  Supplemented County: Yuba County\n",
      "  City: Brownsville \n",
      "  State: CA\n",
      "  Coordinates: (39.4092070163471, -121.304662266888)\n",
      "\n",
      "Record 78439:\n",
      "  Original County: nan\n",
      "  Supplemented County: Yuba County\n",
      "  City: Brownsville \n",
      "  State: CA\n",
      "  Coordinates: (39.4073752168849, -121.304868282922)\n",
      "\n",
      "COLUMN DIFFERENCES:\n",
      "Existing processed data appears to be a subset with 20 columns vs 47 in original\n",
      "Key columns in existing processed data:\n",
      "['_id', 'OBJECTID', '* Damage', 'State', '* CAL FIRE Unit', 'County', '* Incident Name', 'Incident Number (e.g. CAAEU 123456)', 'Incident Start Date', 'Hazard Type'] ...\n",
      "\n",
      "Both supplemented and existing data have complete county information!\n",
      "Existing processed data already filled the same missing counties.\n",
      "\n",
      "FILE SIZES:\n",
      "New supplemented file: 43.3 MB\n",
      "Existing processed file: 24.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Detailed comparison of the datasets\n",
    "print(\"DETAILED COMPARISON:\")\n",
    "\n",
    "# Show sample of counties that were filled by supplementation\n",
    "original_missing = wildfire_df['County'].isna()\n",
    "if original_missing.any():\n",
    "    print(\"\\nSample records where counties were supplemented:\")\n",
    "    sample_indices = wildfire_df[original_missing].index[:5]  # First 5 missing records\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        print(f\"\\nRecord {idx}:\")\n",
    "        print(f\"  Original County: {wildfire_df.loc[idx, 'County']}\")\n",
    "        print(f\"  Supplemented County: {wildfire_supplemented.loc[idx, 'County']}\")\n",
    "        print(f\"  City: {wildfire_df.loc[idx, '* City']}\")\n",
    "        print(f\"  State: {wildfire_df.loc[idx, 'State']}\")\n",
    "        print(f\"  Coordinates: ({wildfire_df.loc[idx, 'Latitude']}, {wildfire_df.loc[idx, 'Longitude']})\")\n",
    "\n",
    "# Compare with existing processed data structure\n",
    "print(f\"\\nCOLUMN DIFFERENCES:\")\n",
    "print(f\"Existing processed data appears to be a subset with {existing_df.shape[1]} columns vs {wildfire_df.shape[1]} in original\")\n",
    "print(f\"Key columns in existing processed data:\")\n",
    "print(existing_df.columns.tolist()[:10], \"...\" if len(existing_df.columns) > 10 else \"\")\n",
    "\n",
    "# Check if existing processed data has same county completeness\n",
    "if 'County' in existing_df.columns:\n",
    "    print(f\"\\nBoth supplemented and existing data have complete county information!\")\n",
    "    print(f\"Existing processed data already filled the same missing counties.\")\n",
    "else:\n",
    "    print(f\"\\nExisting processed data doesn't have County column.\")\n",
    "\n",
    "# File sizes comparison\n",
    "import os\n",
    "new_file_size = os.path.getsize(\"../data/processed/california-wildfires/wildfires_supplemented.csv\")\n",
    "existing_file_size = os.path.getsize(\"../data/processed/california-wildfires/wildfires_with_locations.csv\")\n",
    "\n",
    "print(f\"\\nFILE SIZES:\")\n",
    "print(f\"New supplemented file: {new_file_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Existing processed file: {existing_file_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c6784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildfire-aqi-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
